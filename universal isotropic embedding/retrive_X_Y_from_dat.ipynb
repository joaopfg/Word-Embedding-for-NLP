{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "py37",
   "display_name": "Python (py37)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of words\n",
    "\n",
    "Given a sequence of words $s_1,...s_m$, we perform a $n-gram$\n",
    "\n",
    "# 1. Data processing \n",
    "w.l.o.g. we use a corpora using all words in the input txt.\n",
    "\n",
    "# 2. Models\n",
    "\n",
    "## 2.1. Universal isomorphic embedding (word as graph node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords                                  \n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import word2vec\n",
    "import pandas as pd\n",
    "from numpy import save\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import copy\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "WORD2VEC_VEC = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Will split a text into words and remove stop words\n",
    "    param:\n",
    "         text: string - can be a document or a query\n",
    "\n",
    "    return: \n",
    "         a list of words\n",
    "    \"\"\"\n",
    "    words = None\n",
    "    # the text into a sequence of words and store it in words\n",
    "    words = text.split()\n",
    "    \n",
    "    # # remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "    return words\n",
    "def myindex(cleaned_texts):\n",
    "    \"\"\" use a dictionary (HashMap) for storing the inverted index\n",
    "    param:\n",
    "        cleaned_texts:  \n",
    "\n",
    "    return: \n",
    "        inverted_index: key=word, value=word index in vertex list \n",
    "        vertex_list: list of word index\n",
    "        split_texts: a list of words \n",
    "    \"\"\"\n",
    "    split_texts = tokenize(cleaned_texts) \n",
    "    inverted_index = {}\n",
    "    split_text_set = set(split_texts)\n",
    "    vertex_list = []\n",
    "    for index, word in enumerate(split_text_set):\n",
    "            inverted_index[word] = inverted_index.get(word, [])\n",
    "            inverted_index[word].append(index)\n",
    "            vertex_list.append(word)\n",
    "    return inverted_index, vertex_list, split_texts\n",
    "\n",
    "def generate_edge(word_list, n):\n",
    "    \"\"\" use a sliding window of size n, count the number of pairs as edge weights\n",
    "        input size: n\n",
    "    param:\n",
    "        decoded_texts: a list of vertex index\n",
    "        n: sliding window length\n",
    "\n",
    "    return: \n",
    "        edge_mat: L x L matrix, L being size of word set\n",
    "    \"\"\"\n",
    "    NUM_DISTINCT_WORD = max(word_list) + 1\n",
    "    edge_mat = np.zeros([NUM_DISTINCT_WORD, NUM_DISTINCT_WORD])\n",
    "    pair_list = []\n",
    "\n",
    "    for i in range(len(word_list)-n+1):\n",
    "        pair_list.append(list(permutations(word_list[i:i+n],2))) \n",
    "\n",
    "    for i in range(len(pair_list)):\n",
    "        for it in pair_list[i]:\n",
    "            edge_mat[it[0],it[1]] += 1\n",
    "    return edge_mat \n",
    "\n",
    "\n",
    "\n",
    "def retrive_inverted_index(txt_file_name = 'test800.txt'):\n",
    "    \"\"\" input .txt file name and return inverted index \n",
    "    param:\n",
    "        txt_file_name: txt file name to read\n",
    "\n",
    "    return: \n",
    "        inverted_index: a dictionary (HashMap) for storing the inverted index\n",
    "    \"\"\"\n",
    "\n",
    "\ttext_content = open(txt_file_name, encoding = 'utf-8', errors = 'ignore').read()\n",
    "\t# preprocess \n",
    "\tcleaned_texts = []\n",
    "\tregex = re.compile('[^a-zA-Z]') # filter only letters\n",
    "\tfor innerText in text_content:\n",
    "\t    cleaned_texts.append(regex.sub(' ', innerText))  \n",
    "\n",
    "\t# convert into lowercase\n",
    "\tfor index in range(0, len(cleaned_texts)):\n",
    "\t    cleaned_texts[index] = cleaned_texts[index].lower()\n",
    "\tcleaned_texts = ''.join(cleaned_texts) # convert back to str\n",
    "\t# print('cleaned length',len(cleaned_texts))\n",
    "\n",
    "\n",
    "\tinverted_index, vertex_list, split_texts = myindex(cleaned_texts)\n",
    "\t# encode the whole txt (tokenized) into its vertex number \n",
    "\tdecoded_texts = [inverted_index[word][0] for word in split_texts] # contain vertex index \n",
    "\t# print('decoded length',len(decoded_texts),len(split_texts))\n",
    "\t# print('word set length', len(set(inverted_index)))# generate edge weight matrix \n",
    "\tedge_mat_dict = {}\n",
    "\t# print('decoded_texts length:',len(decoded_texts))\n",
    "\n",
    "    # ####### use sentence as vertex #######\n",
    "    # sentences = sent_tokenize(text_content)\n",
    "    # sentences = clean_sentences(sentences)\n",
    "    # graph_vertices, inverted_index_sentence = get_vertices(sentences)\n",
    "\n",
    "\n",
    "\n",
    "\treturn inverted_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Generate train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map 26 tags to 5 self-defiend tages \n",
    "tag_dict = {'CD':4, 'CC':4, 'DT':4, 'EX':4, 'FW':4, 'IN':4, 'LS':4, 'MD':4, 'WP':4,'WP$':4,'PRP':4,'TO':4, 'UH':4,'WDT':4,'WRB':4,\n",
    "            'JJ':0,'JJS':0,'JJR':0,\n",
    "            'NN':1,'NNS':1,'NNP':1,'NNPS':1,\n",
    "            'RB':2, 'RBR':2, 'RP':2,\n",
    "            'VB':3, 'VBD':3, 'VBG':3, 'VBN':3, 'VBP':3, 'VBZ':3}\n",
    "label_num_to_tag_dict = {0: 'Adjective', \n",
    "                    1: 'Noun',\n",
    "                    2: 'Adverb',\n",
    "                    3: 'Verb',\n",
    "                    4: 'Others'}\n",
    "\n",
    "def retrive_X_Y_all(uie_dat_name, txt_file_name, k):\n",
    "    \"\"\" retrieve data and labels \n",
    "\tInput:\t\n",
    "\t\tuie_dat_name: file name of the .npy file\n",
    "\t\tinverted_index: orginal inevrted index\n",
    "\t\tk: reduced dimension \n",
    "\n",
    "\tReturn:\n",
    "\t\tX_gv: retrived from google vector  \n",
    "\t\tX_uie: retrived from universal isotropic embedding \n",
    "\t\tY: labels (pos tagging)\n",
    "\t\"\"\"\n",
    "    inverted_index = retrive_inverted_index(txt_file_name)\n",
    "    \n",
    "\n",
    "    ########## Google vector ##########\n",
    "    # words google vectors do not contain \n",
    "    error_words = []\n",
    "    for ele in inverted_index.keys():\n",
    "        try:\n",
    "            WORD2VEC_VEC[ele]\n",
    "            # raise ValueError(\"I have raised an Exception\")\n",
    "        except KeyError as exp:\n",
    "            error_words.append(ele)\n",
    "\n",
    "    # new_inverted_index only has what google vector contaion \n",
    "    new_inverted_index = copy.deepcopy(inverted_index) \n",
    "    for ele in error_words:\n",
    "        del new_inverted_index[ele] \n",
    "\n",
    "    unique_texts = list(new_inverted_index.keys())\n",
    "\n",
    "    vec = [WORD2VEC_VEC[ele] for ele in unique_texts]\n",
    "    pca = PCA(n_components=k)\n",
    "    X_wtv = pca.fit_transform(vec) # google vector \n",
    "    print('PCA total explained ratio:', sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "    ########## universal isotropic embedding  ##########\n",
    "    # load data \n",
    "    vec_uie = np.load(uie_dat_name)\n",
    "    # remove col and rows of words not appearing in google vector \n",
    "    temp = np.array(vec_uie)\n",
    "    for word in error_words:\n",
    "        idx = inverted_index[word]\n",
    "        temp = np.delete(temp, idx, axis=0)\n",
    "        temp = np.delete(temp, idx, axis=1)\n",
    "    X_uie = pca.fit_transform(temp) # universal isotropic embedding  \n",
    "    \n",
    "    ########## Retrive labels  ##########\n",
    "    pos_tags0 = nltk.pos_tag(list(new_inverted_index.keys())) # get the pos tag for each vertex\n",
    "    print('original occurance: ',Counter(elem[1] for elem in pos_tags0))\n",
    "    set(ele[1] for ele in pos_tags0)\n",
    "\n",
    "    # check if all predicted label by nltk in tag_dict \n",
    "    for ele in pos_tags0:\n",
    "        if ele[1] not in tag_dict.keys():\n",
    "            print('tag', ele,  'not exists in tag_dict, please modify')\n",
    "            return \n",
    "    # the label after mapping\n",
    "    Y = [tag_dict[ele[1]] for ele in pos_tags0]\n",
    "\n",
    "    if len(X_wtv) != len(X_uie) or len(X_wtv) != len(Y):\n",
    "        print(\"Not same length, X_wtv, X_uie, Y\", len(X_wtv), len(X_uie), len(Y))\n",
    "\n",
    "            \n",
    "    return X_wtv, X_uie, Y, list(new_inverted_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Save train and test data\n",
    "use the data generated and the txt to construct train and test labels for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PCA total explained ratio: 0.17989720993887387\noriginal occurance:  Counter({'NN': 237, 'JJ': 167, 'NNS': 97, 'RB': 85, 'VBD': 44, 'VBP': 44, 'VBG': 28, 'VBN': 25, 'VB': 17, 'VBZ': 17, 'IN': 13, 'JJS': 9, 'MD': 4, 'RBR': 3, 'CD': 3, 'DT': 2, 'PRP': 1, 'WP$': 1, 'JJR': 1, 'WDT': 1})\n799 799 799 799\n"
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "uie_dat_name = 'GDM5_test800.npy'\n",
    "txt_file_name = 'test800.txt'\n",
    "X_wtv, X_uie, Y, words_list = retrive_X_Y_all(uie_dat_name, txt_file_name, k)\n",
    "\n",
    "np.save('X_wtv.npy', X_wtv)\n",
    "np.save('X_uie.npy', X_uie)\n",
    "np.save('Y.npy', Y)\n",
    "np.save('word_list.npy', words_list)\n",
    "\n",
    "print(len(X_wtv), len(X_wtv), len(Y),len(words_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PCA total explained ratio: 0.17488964472028978\noriginal occurance:  Counter({'NN': 418, 'JJ': 252, 'NNS': 145, 'RB': 107, 'VBD': 69, 'VBP': 66, 'VBG': 44, 'VBN': 40, 'VBZ': 26, 'IN': 16, 'VB': 14, 'JJS': 9, 'MD': 6, 'JJR': 4, 'CD': 3, 'PRP': 2, 'DT': 2, 'RBR': 1, 'WP$': 1, 'WDT': 1, 'WRB': 1})\n1227 1227 1227 1227\n"
    }
   ],
   "source": [
    "\n",
    "uie_dat_name = \"GDM3_test1000.npy\"\n",
    "txt_file_name = \"test1000.txt\"\n",
    "X_wtv, X_uie, Y, words_list= retrive_X_Y_all(uie_dat_name, txt_file_name, k)\n",
    "\n",
    "np.save('X_wtv.npy', X_wtv)\n",
    "np.save('X_uie.npy', X_uie)\n",
    "np.save('Y.npy', Y)\n",
    "np.save('word_list.npy', words_list)\n",
    "\n",
    "print(len(X_wtv), len(X_wtv), len(Y),len(words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: it is superising that when dimension is reduced to 10, the preserved information is approximatedly 18%; even if we use reduced dimension to 50, the presevered information is still 50\n",
    "\n",
    "\n",
    "## 2.2. Universal isomorphic embedding (sentence as graph node)\n",
    "- Use sentence as the graph nodes to construct vector \n",
    "- Use the vectors of the sentence to reconstruct word vector using a weighted recombination related to the vertex degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sentences):\n",
    "    tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        token = word_tokenize(sentence)\n",
    "        token = [w.lower() for w in token]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        tokens.append(words)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_vertices(sentences):\n",
    "    graph_vertices = []\n",
    "    inverse_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence_set = set(sentence)\n",
    "        vertex = {}\n",
    "        inverse = {}\n",
    "        for index, word in enumerate(sentence_set):\n",
    "            vertex[word] = index\n",
    "            inverse[index] = word\n",
    "        graph_vertices.append(vertex)\n",
    "        inverse_words.append(inverse)\n",
    "    return graph_vertices, inverse_words\n",
    "\n",
    "def get_words_set(graph_vertices):\n",
    "    words_set = set()\n",
    "    for i in range(len(graph_vertices)):\n",
    "        for key in graph_vertices[i]:\n",
    "            words_set.add(key)\n",
    "    return words_set\n",
    "\n",
    "def get_words_list(graph_vertices):\n",
    "    words_list = []\n",
    "    for i in range(len(graph_vertices)):\n",
    "        words_set = set()\n",
    "        for key in graph_vertices[i]:\n",
    "            words_set.add(key)\n",
    "        words_list.append(words_set)\n",
    "    return words_list\n",
    "\n",
    "def get_decoded_sentences(graph_vertices, sentences):\n",
    "    decoded_sentences = []\n",
    "    count = 0\n",
    "    for vertex in graph_vertices:\n",
    "        decoded_sentence = [vertex[word] for word in sentences[count]]\n",
    "        decoded_sentences.append(decoded_sentence)\n",
    "        count += 1\n",
    "    return decoded_sentences\n",
    "\n",
    "##############################\n",
    "# Modified\n",
    "##############################\n",
    "def generate_edge(word_list, n):\n",
    "    \"\"\"\n",
    "    use a sliding window of size n, count the number of pairs as edge weights\n",
    "    input size: n\n",
    "    return: nxn matrix\n",
    "    \"\"\"\n",
    "    edge_mat = np.zeros([max(word_list)+1, max(word_list)+1])\n",
    "    pair_list = []\n",
    "\n",
    "    for i in range(len(word_list)-n+1):\n",
    "        pair_list.append(list(permutations(word_list[i:i+n],2))) \n",
    "\n",
    "    for i in range(len(pair_list)):\n",
    "        for it in pair_list[i]:\n",
    "            edge_mat[it[0],it[1]] += 1\n",
    "    return edge_mat # (a,b), (b,a) counted twice\n",
    "\n",
    "def get_adj_mats(decoded_sentences, window_size):\n",
    "    adj_mats = []\n",
    "    for decoded_sentence in decoded_sentences:\n",
    "        if len(decoded_sentence) >= 2:\n",
    "            adj_mats.append(generate_edge(decoded_sentence ,window_size))\n",
    "        else:\n",
    "            adj_mats.append(None)\n",
    "    return adj_mats\n",
    "\n",
    "def get_edges(adj_mats):\n",
    "    edges_sets = []\n",
    "    for i in range(len(adj_mats)):\n",
    "        edges = []\n",
    "        for j in range(len(adj_mats[i])):\n",
    "            for z in range(j):\n",
    "                if adj_mats[i][j][z] != 0:\n",
    "                    edges.append((j,z))\n",
    "        edges_sets.append(edges)\n",
    "    return edges_sets\n",
    "\n",
    "def get_degree_contributions(adj_mats, edges_sets):\n",
    "    degree_contributions = []\n",
    "    for i in range(len(edges_sets)):\n",
    "        degree = dict()\n",
    "        for edge in edges_sets[i]:\n",
    "            degree[edge[0]] = 0\n",
    "            degree[edge[1]] = 0\n",
    "        for edge in edges_sets[i]:\n",
    "            degree[edge[0]] += 1\n",
    "            degree[edge[1]] += 1\n",
    "        vertices_set = set()\n",
    "        for edge in edges_sets[i]:\n",
    "            vertices_set.add(edge[0])\n",
    "            vertices_set.add(edge[1])\n",
    "        for vertex in vertices_set:\n",
    "            degree[vertex] /= 2*len(edges_sets[i])\n",
    "        degree_contributions.append(degree)\n",
    "    return degree_contributions\n",
    "\n",
    "def pad(a, MAX_len):\n",
    "    \"\"\" \n",
    "    a: Array to be padded\n",
    "    MAX_len: return shape MAX_len X MAX_len\n",
    "\n",
    "    return result (zero padded matrix with shape=(MAX_len,MAX_len))\n",
    "    \"\"\"\n",
    "\n",
    "    result = np.zeros((MAX_len,MAX_len))\n",
    "    result[:a.shape[0],:a.shape[1]] = a\n",
    "    # pad(a, [MAX_len,MAX_len], len(a)*len(a))\n",
    "    return result\n",
    "\n",
    "def get_word_vector(words_set, words_list, graph_vertices, X_proj, degree_contributions):    \n",
    "    word_vector = dict()\n",
    "    # each word\n",
    "    for word in words_set:\n",
    "        cur_vec = np.zeros(np.shape(X_proj[0]))\n",
    "        sum = 0.0\n",
    "        # iterate each sentence\n",
    "        for i in range(len(words_list)):\n",
    "            # if word in the the sentence \n",
    "            if word in words_list[i]:\n",
    "                # print(i,word)\n",
    "                if len(degree_contributions[i]) != 0:\n",
    "                    cur_vec += degree_contributions[i][graph_vertices[i][word]]*X_proj[i]\n",
    "                    sum += degree_contributions[i][graph_vertices[i][word]]\n",
    "        if sum != 0.0:\n",
    "            cur_vec /= sum\n",
    "        word_vector[word] = cur_vec\n",
    "    return word_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "107 107 107 107\n"
    }
   ],
   "source": [
    "# read file\n",
    "txt_file_name = 'test800.txt'\n",
    "text_content = open(txt_file_name, encoding = 'utf-8', errors = 'ignore').read()\n",
    "sentences = sent_tokenize(text_content)\n",
    "sentences = clean_sentences(sentences)\n",
    "\n",
    "\n",
    "# remove sentences with zero or one word \n",
    "cleaned_sentence = []\n",
    "for ele in sentences:\n",
    "    if not len(ele) <= 1:\n",
    "        cleaned_sentence.append(ele)\n",
    "    else: \n",
    "        # print(ele)\n",
    "        pass\n",
    "        \n",
    "sentences = cleaned_sentence\n",
    "graph_vertices, inverse_words = get_vertices(sentences)\n",
    "words_list = get_words_list(graph_vertices)\n",
    "words_set = get_words_set(graph_vertices)\n",
    "decoded_sentences = get_decoded_sentences(graph_vertices, sentences)\n",
    "\n",
    "adj_mats = get_adj_mats(decoded_sentences, 3)\n",
    "edges_sets = get_edges(adj_mats)\n",
    "degree_contributions = get_degree_contributions(adj_mats, edges_sets)\n",
    "\n",
    "GDM_list = []\n",
    "##################################################\n",
    "# update non-connected vertices with shortest path\n",
    "for i in range(len(adj_mats)):\n",
    "    GDM = np.array(adj_mats[i])\n",
    "    # GDM[GDM == 0] = 1e10\n",
    "    # update non-connected vertices with shortest path\n",
    "    if adj_mats[i] is not None:\n",
    "        GDM[GDM == 0] = 1e30\n",
    "        # dignal = 0\n",
    "        for i in range(len(GDM)):\n",
    "            GDM[i,i] = 0\n",
    "        for z in range(len(GDM)):\n",
    "            for u in range(len(GDM)):\n",
    "                for v in range(len(GDM)): \n",
    "                    if GDM[u,v] > GDM[u,z] + GDM[z,v]:\n",
    "                        GDM[u,v] = GDM[u,z] + GDM[z,v]   \n",
    "    else:\n",
    "        GDM = None\n",
    "\n",
    "    GDM_list.append(GDM)\n",
    "\n",
    "# zero padding s.t. all sentence vector has same shape \n",
    "MAX_len = 0\n",
    "cleaned_GDM_list = []\n",
    "clean_idx_list = []\n",
    "for ele in GDM_list:\n",
    "    if ele is None:\n",
    "        clean_idx_list.append(False)\n",
    "        pass\n",
    "    else:\n",
    "        clean_idx_list.append(False)\n",
    "        cleaned_GDM_list.append(ele)\n",
    "        if len(ele) > MAX_len:\n",
    "            MAX_len = len(ele)\n",
    "\n",
    "# list after zero padding \n",
    "GDM_pedded_list = []\n",
    "\n",
    "for i in range(len(cleaned_GDM_list)):\n",
    "    GDM_pedded_list.append(pad(cleaned_GDM_list[i], MAX_len))\n",
    "\n",
    "print(len(GDM_pedded_list), len(graph_vertices), len(degree_contributions), len(clean_idx_list))\n",
    "\n",
    "word_vector = get_word_vector(words_set, words_list, graph_vertices, GDM_pedded_list, degree_contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The word not in the constructed dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cannot\nself\ncounter\neducate\nmonkeys\nninety\nprice\nserving\nco\npowder\nbelieve\nhigh\nborn\nbalance\ncurrent\nprices\nreliance\noperate\nholders\ncloth\nhouses\nalms\n"
    }
   ],
   "source": [
    "word_list = np.load('word_list.npy')\n",
    "for key in word_vector.keys():\n",
    "    word_vector[key] = list(word_vector[key].flat)\n",
    "    \n",
    "X_final = []\n",
    "word_mising = []\n",
    "for word in word_list:\n",
    "    if word in word_vector.keys():\n",
    "        X_final.append(word_vector[word])\n",
    "    else:\n",
    "         X_final.append(np.zeros(np.shape(word_vector['questions'])))\n",
    "         word_mising.append(word)\n",
    "         print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PCA total explained ratio: 0.9541151275871456\n"
    },
    {
     "data": {
      "text/plain": "799"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save file \n",
    "k = 10\n",
    "pca = PCA(n_components=k)\n",
    "X_uiesg = pca.fit_transform(X_final) # google vector \n",
    "print('PCA total explained ratio:', sum(pca.explained_variance_ratio_))\n",
    "\n",
    "np.save('X_uiesg.npy',X_uiesg)\n",
    "\n",
    "len(X_uiesg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation \n",
    "One possibility of a slight better performance as opposed one use words as node to construct word vector is that after PCA, the more information is presevered 95% vs. 18%. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1.2. Kmeans for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# X = word_vec_dict[2,2]\n",
    "n_clusters = 5\n",
    "Y_pred_dict = {} # key: n,k ;stroe all predictions with sliding window length n, reduced dimenion k\n",
    "\n",
    "Y_w2v_pred_dict = {}\n",
    "\n",
    "# word2node\n",
    "# # for n in N_RANGE:\n",
    "# for k in K_RANGE:\n",
    "X = X_uie\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "Y_uis_pred= kmeans.labels_\n",
    "\n",
    "# word2vec\n",
    "# for k in K_RANGE:\n",
    "X = X_wtv\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "Y_w2v_pred = kmeans.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Y),len(Y_pred_dict[n,k]),len(X[:,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_pred_dim23(n, k, word_vec_dict, Y_pred_dict, Y_w2v_dict):\n",
    "    \"\"\" plot the prediction of Kmeans \n",
    "    Input\n",
    "        n: sliding window length\n",
    "        k: dim reduced using PCA\n",
    "        word_vec_dict: key = [n,k] value = X with dim=[len(word_set),k] \n",
    "        Y_pred_dict: key = [n,k] value = Y_pred with dim=[len(word_set),k] \n",
    "        \n",
    "    Return \n",
    "        two plots\n",
    "    \"\"\"\n",
    "    X =  word_vec_dict[n,k]\n",
    "\n",
    "    n_clusters = max(Y)+1\n",
    "\n",
    "    if k == 2:\n",
    "        plt.figure(figsize = (15, 5))\n",
    "        for cluster in range(n_clusters):\n",
    "            idx = (np.array(Y) == cluster)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], s=50, label = label_num_to_tag_dict[cluster])\n",
    "        plt.title('Reality')\n",
    "        plt.legend()\n",
    "        plt.grid(1)\n",
    "\n",
    "        plt.figure(figsize = (15, 5))\n",
    "        for cluster in range(n_clusters):\n",
    "            idx = (np.array(Y_w2v_pred_dict[k]) == cluster)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], s=50, label = label_num_to_tag_dict[cluster])\n",
    "        plt.title('word2vec')\n",
    "        plt.legend()\n",
    "        plt.grid(1)\n",
    "\n",
    "        plt.figure(figsize = (15, 5))\n",
    "        for cluster in range(n_clusters):\n",
    "            idx = (np.array(Y_pred_dict[n,k]) == cluster)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], s=50, label = label_num_to_tag_dict[cluster])\n",
    "        plt.title('word2node')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(1)\n",
    "\n",
    "    elif k == 3:\n",
    "        fig1 = plt.figure(figsize = (15, 5))\n",
    "        ax = Axes3D(fig1)\n",
    "        for cluster in range(n_clusters):\n",
    "            idx = (np.array(Y) == cluster)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], X[idx, 2], marker = 'o', label = label_num_to_tag_dict[cluster])\n",
    "        plt.title('Reality')\n",
    "        plt.legend()\n",
    "        plt.grid(1)\n",
    "\n",
    "        fig2 = plt.figure(figsize = (15, 5))\n",
    "        ax = Axes3D(fig2)\n",
    "        for cluster in range(n_clusters):\n",
    "            idx = (np.array(Y_w2v_pred_dict[k]) == cluster)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], X[idx, 2], marker = 'o', label = label_num_to_tag_dict[cluster])\n",
    "        plt.title('word2vec')\n",
    "        plt.legend()\n",
    "        plt.grid(1)\n",
    "\n",
    "        fig3 = plt.figure(figsize = (15, 5))\n",
    "        ax = Axes3D(fig3)\n",
    "        for cluster in range(n_clusters):\n",
    "            idx = (np.array(Y_pred_dict[n,k]) == cluster)\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], X[idx, 2], marker = 'o', label = label_num_to_tag_dict[cluster])\n",
    "        plt.title('word2node')\n",
    "        plt.legend()\n",
    "        plt.grid(1)\n",
    "\n",
    "    else:\n",
    "        print('Cannot deal with k>=3')\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 2\n",
    "k = 2\n",
    "plot_pred_dim23(n, k, word_vec_dict, Y_pred_dict, Y_w2v_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 2\n",
    "k = 3\n",
    "plot_pred_dim23(n, k, word_vec_dict, Y_pred_dict, Y_w2v_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2. Setiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1. Graph Construction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mat = edge_mat_dict[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = text.split()\n",
    "words = [word for word in words if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G0 = nx.Graph()\n",
    "G0.add_nodes_from(range(len(vertex_list)))\n",
    "\n",
    "# flatten the matrix as [x_idx,y_idx, value]\n",
    "XX,YY = np.meshgrid(np.arange(edge_mat.shape[1]),np.arange(edge_mat.shape[0]))\n",
    "idx = [edge_mat.ravel() != 0]\n",
    "flattened_mat = np.vstack((XX.ravel()[idx], YY.ravel()[idx], edge_mat.ravel()[idx])).T\n",
    "flattened_mat = [tuple(ele) for ele in flattened_mat]\n",
    "\n",
    "\n",
    "G0.add_weighted_edges_from(flattened_mat)\n",
    "\n",
    "\n",
    "# nx.all_pairs_dijkstra(G0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> for (u, v, wt) in G0.edges.data('weight'):\n",
    "...     if wt > 0.5: print('(%d, %d, %.3f)' % (u, v, wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dict(nx.all_pairs_shortest_path(G0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDM_new = np.zeros(np.shape(GDM_dict[10]))\n",
    "for i in range(len(inverted_index.keys())):\n",
    "    for j in range(len(inverted_index.keys())):\n",
    "        GDM_new[i,j] = dict_ew[i][0][j]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDM_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n100\n"
    }
   ],
   "source": [
    "for source in range(len(inverted_index.keys())):\n",
    "    if source % 100 == 0:\n",
    "        print(source) \n",
    "    for target in range(source,len(inverted_index.keys())):\n",
    "        if source != target:\n",
    "            # shortest path from source to target \n",
    "            # print(nx.dijkstra_path_length(G0,source,target))\n",
    "            # sp = nx.shortest_path_length(G0,source=source,target=target)\n",
    "            # print(sp)\n",
    "            G0.add_edge(source, target, weight=nx.dijkstra_path_length(G0,source,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<networkx.classes.graph.Graph at 0x1b4c473400>"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_numpy_matrix(G0, nodelist=[range(len(inverted_index.keys()))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edge(1, 2, weight=3)\n",
    "temp = G.get_edge_data(1,2)['weight']\n",
    "temp += 5\n",
    "G.add_edge(1, 2, weight= temp)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(nx.all_pairs_dijkstra(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(inverted_index.keys())all_pairs_dijkstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,60))\n",
    "plt.show()\n",
    "nx.draw(G0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "G.add_edges_from([(1,2),(1,3)])\n",
    "G.add_node(\"spam\")      \n",
    "nx.connected_components(G)\n",
    "\n",
    "nx.draw(G)\n",
    "nx.draw_random(G)\n",
    "nx.draw_circular(G)\n",
    "nx.draw_spectral(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}